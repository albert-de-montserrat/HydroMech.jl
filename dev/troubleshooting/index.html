<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Troubleshooting · HydroMech.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.svg" alt="HydroMech.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">HydroMech.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><span class="tocitem">Getting started</span><ul><li><a class="tocitem" href="../overview/">Overview</a></li><li><a class="tocitem" href="../methodology/">Methodology</a></li></ul></li><li><span class="tocitem">Solvers</span><ul><li><a class="tocitem" href="../2D-hydro-mechanical-solver/">2D-Hydro-mechanical solver</a></li><li><a class="tocitem" href="../3D-hydro-mechanical-solver/">3D-Hydro-mechanical solver</a></li></ul></li><li><span class="tocitem">Concepts</span><ul><li><a class="tocitem" href="../pseudo-transient-method/">Pseudo Transient Method</a></li><li><a class="tocitem" href="../iteration-parameters/">Iteration Parameters</a></li><li><a class="tocitem" href="../stiffness-of-pdes/">Stiffness of PDEs</a></li><li><a class="tocitem" href="../eigenvalue-problem/">Eigenvalue Problem</a></li><li><a class="tocitem" href="../dispersion-analysis/">Dispersion Analysis</a></li><li><a class="tocitem" href="../von-neumann-stability-analysis/">Von Neumann Stability Analysis</a></li><li><a class="tocitem" href="../computational-earthquake-physics/">Computational Earthquake Physics</a></li></ul></li><li><a class="tocitem" href="../performance/">Performance</a></li><li><a class="tocitem" href="../benchmarks/">Benchmarks</a></li><li><a class="tocitem" href="../visualization/">Visualization</a></li><li><span class="tocitem">Development</span><ul><li><a class="tocitem" href="../roadmap/">Roadmap</a></li><li><a class="tocitem" href="../timeline/">Time line</a></li><li class="is-active"><a class="tocitem" href>Troubleshooting</a><ul class="internal"><li><a class="tocitem" href="#Problem:-CUDA-aware-MPI-does-not-work-on-cluster"><span>Problem: CUDA-aware MPI does not work on cluster</span></a></li><li><a class="tocitem" href="#Problem:-No-solutions-after-certain-iteration-count"><span>Problem:  No solutions after certain iteration count</span></a></li><li><a class="tocitem" href="#Problem:-Overhead-brought-by-using-MetaHydroMech.jl"><span>Problem:  Overhead brought by using MetaHydroMech.jl</span></a></li></ul></li></ul></li><li><span class="tocitem">Reference</span><ul><li><a class="tocitem" href="../hydromech/">HydroMech.jl</a></li><li><input class="collapse-toggle" id="menuitem-9-2" type="checkbox"/><label class="tocitem" for="menuitem-9-2"><span class="docs-label">PTsolvers/JustRelax.jl</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../justrelax-overview/">Overview</a></li><li><input class="collapse-toggle" id="menuitem-9-2-2" type="checkbox"/><label class="tocitem" for="menuitem-9-2-2"><span class="docs-label">Source code</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../justrelax-modules/">Modules</a></li><li><a class="tocitem" href="../justrelax-types/">Types</a></li></ul></li><li><a class="tocitem" href="../justrelax-Miniapps/">Miniapps</a></li><li><a class="tocitem" href="../justrelax-Testing/">Testing</a></li></ul></li><li><a class="tocitem" href="../parallelstencil/">ParallelStencil</a></li><li><a class="tocitem" href="../juliageodynamics/">JuliaGeodynamics</a></li></ul></li><li><a class="tocitem" href="../license/">License</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Development</a></li><li class="is-active"><a href>Troubleshooting</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Troubleshooting</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/youwuyou/HydroMech.jl/blob/main/docs/src/troubleshooting.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Troubleshooting"><a class="docs-heading-anchor" href="#Troubleshooting">Troubleshooting</a><a id="Troubleshooting-1"></a><a class="docs-heading-anchor-permalink" href="#Troubleshooting" title="Permalink"></a></h1><h2 id="Problem:-CUDA-aware-MPI-does-not-work-on-cluster"><a class="docs-heading-anchor" href="#Problem:-CUDA-aware-MPI-does-not-work-on-cluster">Problem: CUDA-aware MPI does not work on cluster</a><a id="Problem:-CUDA-aware-MPI-does-not-work-on-cluster-1"></a><a class="docs-heading-anchor-permalink" href="#Problem:-CUDA-aware-MPI-does-not-work-on-cluster" title="Permalink"></a></h2><p>The MPI library we are using is the mpich of version 4.0.2, which was installed using spack, and is cuda-aware. Following is the description of the module.</p><h4 id="MPICH-4.0.2-itself-is-working-on-the-cluster"><a class="docs-heading-anchor" href="#MPICH-4.0.2-itself-is-working-on-the-cluster">MPICH-4.0.2 itself is working on the cluster</a><a id="MPICH-4.0.2-itself-is-working-on-the-cluster-1"></a><a class="docs-heading-anchor-permalink" href="#MPICH-4.0.2-itself-is-working-on-the-cluster" title="Permalink"></a></h4><details><summary>See the content of the module here! </summary><pre><code class="language-bash hljs">#%Module1.0
## Module file created by spack (https://github.com/spack/spack) on 2022-11-02 14:07:50.241360
##
## mpich@4.0.2%gcc@11.2.0~argobots+cuda+fortran+hcoll+hwloc+hydra+libxml2+pci~rocm+romio+slurm~two_level_namespace~vci~verbs+wrapperrpath cuda_arch=70 datatype-engine=auto device=ch4 netmod=ucx patches=d4c0e99 pmi=pmi2 arch=linux-rocky8-zen2/nny23wg
##
## Configure options: --disable-silent-rules --enable-shared --with-pm=hydra --enable-romio --without-ibverbs --enable-wrapper-rpath=yes --with-yaksa=/home/wyou/spack/opt/spack/linux-rocky8-zen2/gcc-11.2.0/yaksa-0.2-nis43l4sezzmot2oyaxzvj3zrtmmhbmr --with-hwloc=/home/wyou/spack/opt/spack/linux-rocky8-zen2/gcc-11.2.0/hwloc-2.8.0-s4ixhugy7todz4yqzefnyqwrihwp735v --with-slurm=yes --with-slurm-include=/cm/shared/apps/slurm/21.08.8/include --with-slurm-lib=/cm/shared/apps/slurm/21.08.8/lib --with-pmi=pmi2/simple --with-cuda=/home/wyou/spack/opt/spack/linux-rocky8-zen2/gcc-11.2.0/cuda-11.8.0-kh2t6kpkp54oq756ie2x3r7ywiwozwv7 --without-hip --with-device=ch4:ucx --with-ucx=/usr --enable-libxml2 --with-datatye-engine=auto --with-hcoll=/opt/mellanox/hcoll
##


module-whatis &quot;MPICH is a high performance and widely portable implementation of the Message Passing Interface (MPI) standard.&quot;

proc ModulesHelp { } {
puts stderr &quot;MPICH is a high performance and widely portable implementation of the&quot;
puts stderr &quot;Message Passing Interface (MPI) standard.&quot;
}


prepend-path PATH &quot;/home/wyou/spack/opt/spack/linux-rocky8-zen2/gcc-11.2.0/mpich-4.0.2-nny23wg4ku5utyrhdtw7ccz3nlgqmmsg/./bin&quot;
prepend-path MANPATH &quot;/home/wyou/spack/opt/spack/linux-rocky8-zen2/gcc-11.2.0/mpich-4.0.2-nny23wg4ku5utyrhdtw7ccz3nlgqmmsg/./share/man&quot;
prepend-path PKG_CONFIG_PATH &quot;/home/wyou/spack/opt/spack/linux-rocky8-zen2/gcc-11.2.0/mpich-4.0.2-nny23wg4ku5utyrhdtw7ccz3nlgqmmsg/./lib/pkgconfig&quot;
prepend-path CMAKE_PREFIX_PATH &quot;/home/wyou/spack/opt/spack/linux-rocky8-zen2/gcc-11.2.0/mpich-4.0.2-nny23wg4ku5utyrhdtw7ccz3nlgqmmsg/./&quot;
setenv MPICC &quot;/home/wyou/spack/opt/spack/linux-rocky8-zen2/gcc-11.2.0/mpich-4.0.2-nny23wg4ku5utyrhdtw7ccz3nlgqmmsg/bin/mpicc&quot;
setenv MPICXX &quot;/home/wyou/spack/opt/spack/linux-rocky8-zen2/gcc-11.2.0/mpich-4.0.2-nny23wg4ku5utyrhdtw7ccz3nlgqmmsg/bin/mpic++&quot;
setenv MPIF77 &quot;/home/wyou/spack/opt/spack/linux-rocky8-zen2/gcc-11.2.0/mpich-4.0.2-nny23wg4ku5utyrhdtw7ccz3nlgqmmsg/bin/mpif77&quot;
setenv MPIF90 &quot;/home/wyou/spack/opt/spack/linux-rocky8-zen2/gcc-11.2.0/mpich-4.0.2-nny23wg4ku5utyrhdtw7ccz3nlgqmmsg/bin/mpif90&quot;</code></pre></details><p>In order to check that the MPICH we are using is indeed working properly, we run a small device-to-device benchmark on the compute node which has 4 V100-GPUs attached to it.</p><pre><code class="language-bash hljs"># making sure we are running the benchmark using the correct library
[wyou@racklette1 ~]$ which mpirun
~/spack/opt/spack/linux-rocky8-zen2/gcc-11.2.0/mpich-4.0.2-nny23wg4ku5utyrhdtw7ccz3nlgqmmsg/bin/mpirun

# perform benchmark on gpus
[wyou@racklette1 ~]$ srun -n 4 osu_alltoall -d cuda D D 
Warning: OMB could not identify the local rank of the process.
         This can lead to multiple processes using the same GPU.
         Please use the get_local_rank script in the OMB repo for this.
Warning: OMB could not identify the local rank of the process.
         This can lead to multiple processes using the same GPU.
         Please use the get_local_rank script in the OMB repo for this.
Warning: OMB could not identify the local rank of the process.
Warning: OMB could not identify the local rank of the process.
         This can lead to multiple processes using the same GPU.
         Please use the get_local_rank script in the OMB repo for this.
         This can lead to multiple processes using the same GPU.
         Please use the get_local_rank script in the OMB repo for this.

# OSU MPI-CUDA All-to-All Personalized Exchange Latency Test v5.9
# Size       Avg Latency(us)
1                    1281.12
2                    1289.98
4                    1282.38
8                    1287.97
16                   1284.78
32                   1288.85
64                   1285.30
128                  1288.92
256                  1279.70
512                  1288.87
1024                 1285.93
2048                 1290.65
4096                 1289.12
8192                 1298.43
16384                 720.97
32768                 723.08
65536                 727.48
131072                740.52
262144                747.90
524288               1341.46
1048576               788.64</code></pre><h4 id="CUDA-aware-MPI-failed-using-julia"><a class="docs-heading-anchor" href="#CUDA-aware-MPI-failed-using-julia">CUDA-aware MPI failed using julia</a><a id="CUDA-aware-MPI-failed-using-julia-1"></a><a class="docs-heading-anchor-permalink" href="#CUDA-aware-MPI-failed-using-julia" title="Permalink"></a></h4><p>After making sure that the MPI library is cuda-aware and works as expected, we now check on the usage of it using julia. And somehow <code>MPI.Init()</code> does not even work.</p><pre><code class="language-julia hljs">julia&gt; using MPI

julia&gt; MPI.MPI_LIBRARY_VERSION_STRING
&quot;MPICH Version:\t4.0.2\nMPICH Release date:\tThu Apr  7 12:34:45 CDT 2022\nMPICH ABI:\t14:2:2\nMPICH Device:\tch4:ucx\nMPICH configure:\t--prefix=/home/wyou/spack/opt/spack/linux-rocky8-zen2/gcc-11.2.0/mpich-4.0.2-nny23wg4ku5utyrhdtw7ccz3nlgqmmsg --disable-silent-rules --enable-shared --with-pm=hydra --enable-romio --without-ibverbs --enable-wrapper-rpath=yes --with-yaksa=/home/wyou/&quot; ⋯ 502 bytes ⋯ &quot;-with-ucx=/usr --enable-libxml2 --with-datatye-engine=auto --with-hcoll=/opt/mellanox/hcoll\nMPICH CC:\t/home/wyou/spack/lib/spack/env/gcc/gcc    -O2\nMPICH CXX:\t/home/wyou/spack/lib/spack/env/gcc/g++   -O2\nMPICH F77:\t/home/wyou/spack/lib/spack/env/gcc/gfortran -fallow-argument-mismatch  -O2\nMPICH FC:\t/home/wyou/spack/lib/spack/env/gcc/gfortran -fallow-argument-mismatch  -O2\n&quot;

julia&gt; MPI.Init()
Abort(672779791): Fatal error in internal_Init: Other MPI error, error stack:
internal_Init(59)....: MPI_Init(argc=(nil), argv=(nil)) failed
MPII_Init_thread(209): 
MPID_Init(359).......: 
MPIR_pmi_init(141)...: PMI2_Job_GetId returned 14</code></pre><h4 id="MPI-works-in-julia-if-using-mpiexecjl-(but-only-for-helloworld)"><a class="docs-heading-anchor" href="#MPI-works-in-julia-if-using-mpiexecjl-(but-only-for-helloworld)">MPI works in julia if using <code>mpiexecjl</code> (but only for helloworld)</a><a id="MPI-works-in-julia-if-using-mpiexecjl-(but-only-for-helloworld)-1"></a><a class="docs-heading-anchor-permalink" href="#MPI-works-in-julia-if-using-mpiexecjl-(but-only-for-helloworld)" title="Permalink"></a></h4><p>But surprisingly a simple gpu <code>hello_world</code> program works, in which we also ask it to print out the MPI version it is using after <code>using MPI</code> and it has the following output.</p><details><summary> See the hello_world_gpu script here!</summary><pre><code class="language-bash hljs"># Julia MPI &quot;Hello world&quot; code

using MPI, CUDA
MPI.Init()


comm = MPI.COMM_WORLD
me   = MPI.Comm_rank(comm)

# select device

# COMM_TYPE_SHARED splits the communicator into subcommunicators
# each of which can create a shared memory region

comm_l = MPI.Comm_split_type(comm, MPI.MPI_COMM_TYPE_SHARED, me)
me_l   = MPI.Comm_rank(comm_l)   # per node numbering to obtain the GPU ID
GPU_ID = CUDA.device!(me_l)      # set specific CUDA device

if me_l == 0
  println(MPI.MPI_LIBRARY_VERSION_STRING)
end

sleep(0.1me)
println(&quot;Hello world, I am $(me) of $(MPI.Comm_size(comm)) using $(GPU_ID)&quot;)
MPI.Barrier(comm)</code></pre></details><details><summary> See the output of the helloworkd_gpu, which works!</summary><pre><code class="language-bash hljs">[wyou@racklette1 l8_scripts]$ mpiexecjl -n 4 julia --project l8_hello_mpi_gpu.jl 
MPICH Version:	4.0.2
MPICH Release date:	Thu Apr  7 12:34:45 CDT 2022
MPICH ABI:	14:2:2
MPICH Device:	ch4:ucx
MPICH configure:	--prefix=/home/wyou/spack/opt/spack/linux-rocky8-zen2/gcc-11.2.0/mpich-4.0.2-nny23wg4ku5utyrhdtw7ccz3nlgqmmsg --disable-silent-rules --enable-shared --with-pm=hydra --enable-romio --without-ibverbs --enable-wrapper-rpath=yes --with-yaksa=/home/wyou/spack/opt/spack/linux-rocky8-zen2/gcc-11.2.0/yaksa-0.2-nis43l4sezzmot2oyaxzvj3zrtmmhbmr --with-hwloc=/home/wyou/spack/opt/spack/linux-rocky8-zen2/gcc-11.2.0/hwloc-2.8.0-s4ixhugy7todz4yqzefnyqwrihwp735v --with-slurm=yes --with-slurm-include=/cm/shared/apps/slurm/21.08.8/include --with-slurm-lib=/cm/shared/apps/slurm/21.08.8/lib --with-pmi=pmi2/simple --with-cuda=/home/wyou/spack/opt/spack/linux-rocky8-zen2/gcc-11.2.0/cuda-11.8.0-kh2t6kpkp54oq756ie2x3r7ywiwozwv7 --without-hip --with-device=ch4:ucx --with-ucx=/usr --enable-libxml2 --with-datatye-engine=auto --with-hcoll=/opt/mellanox/hcoll
MPICH CC:	/home/wyou/spack/lib/spack/env/gcc/gcc    -O2
MPICH CXX:	/home/wyou/spack/lib/spack/env/gcc/g++   -O2
MPICH F77:	/home/wyou/spack/lib/spack/env/gcc/gfortran -fallow-argument-mismatch  -O2
MPICH FC:	/home/wyou/spack/lib/spack/env/gcc/gfortran -fallow-argument-mismatch  -O2

Hello world, I am 0 of 4 using CuDevice(0)
Hello world, I am 1 of 4 using CuDevice(1)
Hello world, I am 2 of 4 using CuDevice(2)
Hello world, I am 3 of 4 using CuDevice(3)</code></pre></details><p>However the <code>alltoall_test_cuda.jl</code> script failed</p><details><summary>See the `alltoall_test_cuda.jl` script here! </summary><pre><code class="language-julia hljs">using MPI
using CUDA

MPI.Init()
comm = MPI.COMM_WORLD
rank = MPI.Comm_rank(comm)
size = MPI.Comm_size(comm)
dst = mod(rank+1, size)
src = mod(rank-1, size)

println(&quot;rank=$rank, size=$size, dst=$dst, src=$src&quot;)
N = 4
send_mesg = CuArray{Float64}(undef, N)
recv_mesg = CuArray{Float64}(undef, N)
fill!(send_mesg, Float64(rank))
#rreq = MPI.Irecv!(recv_mesg, src,  src+32, comm)
MPI.Sendrecv!(send_mesg, dst, 0, recv_mesg, src, 0, comm)
println(&quot;recv_mesg on proc $rank: $recv_mesg&quot;)
MPI.Finalize()</code></pre></details><details><summary>See the error message here! </summary><pre><code class="language-bash hljs">[wyou@racklette1 debug]$ ls
alltoall_test_cuda.jl  mpi_communication.jl  mpi_cuda_sendrecv.jl  runme.sh  test.jl
[wyou@racklette1 debug]$ vim alltoall_test_cuda.jl 
[wyou@racklette1 debug]$ mpiexecjl -n 4 julia --project alltoall_test_cuda.jl 
rank=2, size=4, dst=3, src=1
rank=0, size=4, dst=1, src=3
rank=3, size=4, dst=0, src=2
rank=1, size=4, dst=2, src=0

signal (11): Segmentation fault
in expression starting at /home/wyou/misc/git-julia/pde-on-gpu-wu/lecture8/scripts/debug/alltoall_test_cuda.jl:17
__memmove_avx_unaligned at /lib64/libc.so.6 (unknown line)
uct_mm_ep_am_short at /usr/lib64/libuct.so.0 (unknown line)
ucp_tag_send_nbx at /lib64/libucp.so.0 (unknown line)
MPIDI_UCX_send.constprop.0.isra.0 at /home/wyou/spack/opt/spack/linux-rocky8-zen2/gcc-11.2.0/mpich-4.0.2-nny23wg4ku5utyrhdtw7ccz3nlgqmmsg/lib/libmpi.so (unknown line)
MPIR_Sendrecv_impl at /home/wyou/spack/opt/spack/linux-rocky8-zen2/gcc-11.2.0/mpich-4.0.2-nny23wg4ku5utyrhdtw7ccz3nlgqmmsg/lib/libmpi.so (unknown line)
MPI_Sendrecv at /home/wyou/spack/opt/spack/linux-rocky8-zen2/gcc-11.2.0/mpich-4.0.2-nny23wg4ku5utyrhdtw7ccz3nlgqmmsg/lib/libmpi.so (unknown line)
Sendrecv! at /home/wyou/.julia/packages/MPI/08SPr/src/pointtopoint.jl:380 [inlined]
Sendrecv! at /home/wyou/.julia/packages/MPI/08SPr/src/pointtopoint.jl:389
unknown function (ip: 0x1554fa9e6a8d)
_jl_invoke at /buildworker/worker/package_linux64/build/src/gf.c:2247 [inlined]
jl_apply_generic at /buildworker/worker/package_linux64/build/src/gf.c:2429
jl_apply at /buildworker/worker/package_linux64/build/src/julia.h:1788 [inlined]
do_call at /buildworker/worker/package_linux64/build/src/interpreter.c:126
eval_value at /buildworker/worker/package_linux64/build/src/interpreter.c:215
eval_stmt_value at /buildworker/worker/package_linux64/build/src/interpreter.c:166 [inlined]
eval_body at /buildworker/worker/package_linux64/build/src/interpreter.c:587
jl_interpret_toplevel_thunk at /buildworker/worker/package_linux64/build/src/interpreter.c:731
jl_toplevel_eval_flex at /buildworker/worker/package_linux64/build/src/toplevel.c:885
jl_toplevel_eval_flex at /buildworker/worker/package_linux64/build/src/toplevel.c:830
jl_toplevel_eval_in at /buildworker/worker/package_linux64/build/src/toplevel.c:944

signal (11): Segmentation fault
in expression starting at /home/wyou/misc/git-julia/pde-on-gpu-wu/lecture8/scripts/debug/alltoall_test_cuda.jl:17
__memmove_avx_unaligned at /lib64/libc.so.6 (unknown line)
uct_mm_ep_am_short at /usr/lib64/libuct.so.0 (unknown line)
ucp_tag_send_nbx at /lib64/libucp.so.0 (unknown line)
MPIDI_UCX_send.constprop.0.isra.0 at /home/wyou/spack/opt/spack/linux-rocky8-zen2/gcc-11.2.0/mpich-4.0.2-nny23wg4ku5utyrhdtw7ccz3nlgqmmsg/lib/libmpi.so (unknown line)
MPIR_Sendrecv_impl at /home/wyou/spack/opt/spack/linux-rocky8-zen2/gcc-11.2.0/mpich-4.0.2-nny23wg4ku5utyrhdtw7ccz3nlgqmmsg/lib/libmpi.so (unknown line)
MPI_Sendrecv at /home/wyou/spack/opt/spack/linux-rocky8-zen2/gcc-11.2.0/mpich-4.0.2-nny23wg4ku5utyrhdtw7ccz3nlgqmmsg/lib/libmpi.so (unknown line)
Sendrecv! at /home/wyou/.julia/packages/MPI/08SPr/src/pointtopoint.jl:380 [inlined]
Sendrecv! at /home/wyou/.julia/packages/MPI/08SPr/src/pointtopoint.jl:389
unknown function (ip: 0x1554fa9e6b1d)
_jl_invoke at /buildworker/worker/package_linux64/build/src/gf.c:2247 [inlined]
jl_apply_generic at /buildworker/worker/package_linux64/build/src/gf.c:2429
jl_apply at /buildworker/worker/package_linux64/build/src/julia.h:1788 [inlined]
do_call at /buildworker/worker/package_linux64/build/src/interpreter.c:126
eval_value at /buildworker/worker/package_linux64/build/src/interpreter.c:215
eval_stmt_value at /buildworker/worker/package_linux64/build/src/interpreter.c:166 [inlined]
eval_body at /buildworker/worker/package_linux64/build/src/interpreter.c:587
jl_interpret_toplevel_thunk at /buildworker/worker/package_linux64/build/src/interpreter.c:731
jl_toplevel_eval_flex at /buildworker/worker/package_linux64/build/src/toplevel.c:885
jl_toplevel_eval_flex at /buildworker/worker/package_linux64/build/src/toplevel.c:830
jl_toplevel_eval_in at /buildworker/worker/package_linux64/build/src/toplevel.c:944
eval at ./boot.jl:373 [inlined]
include_string at ./loading.jl:1196
_jl_invoke at /buildworker/worker/package_linux64/build/src/gf.c:2247 [inlined]
jl_apply_generic at /buildworker/worker/package_linux64/build/src/gf.c:2429
_include at ./loading.jl:1253
include at ./Base.jl:418
_jl_invoke at /buildworker/worker/package_linux64/build/src/gf.c:2247 [inlined]
jl_apply_generic at /buildworker/worker/package_linux64/build/src/gf.c:2429
exec_options at ./client.jl:292
eval at ./boot.jl:373 [inlined]
include_string at ./loading.jl:1196
_jl_invoke at /buildworker/worker/package_linux64/build/src/gf.c:2247 [inlined]
jl_apply_generic at /buildworker/worker/package_linux64/build/src/gf.c:2429
_start at ./client.jl:495
_include at ./loading.jl:1253
jfptr__start_22567.clone_1 at /cm/shared/apps/easybuild-apps/software/Julia/1.7.3-linux-x86_64/lib/julia/sys.so (unknown line)
_jl_invoke at /buildworker/worker/package_linux64/build/src/gf.c:2247 [inlined]
jl_apply_generic at /buildworker/worker/package_linux64/build/src/gf.c:2429
include at ./Base.jl:418
_jl_invoke at /buildworker/worker/package_linux64/build/src/gf.c:2247 [inlined]
jl_apply_generic at /buildworker/worker/package_linux64/build/src/gf.c:2429
jl_apply at /buildworker/worker/package_linux64/build/src/julia.h:1788 [inlined]
true_main at /buildworker/worker/package_linux64/build/src/jlapi.c:559
jl_repl_entrypoint at /buildworker/worker/package_linux64/build/src/jlapi.c:701
main at julia (unknown line)
__libc_start_main at /lib64/libc.so.6 (unknown line)
unknown function (ip: 0x400808)
Allocations: 35530272 (Pool: 35518374; Big: 11898); GC: 34
exec_options at ./client.jl:292
_start at ./client.jl:495
jfptr__start_22567.clone_1 at /cm/shared/apps/easybuild-apps/software/Julia/1.7.3-linux-x86_64/lib/julia/sys.so (unknown line)
_jl_invoke at /buildworker/worker/package_linux64/build/src/gf.c:2247 [inlined]
jl_apply_generic at /buildworker/worker/package_linux64/build/src/gf.c:2429
jl_apply at /buildworker/worker/package_linux64/build/src/julia.h:1788 [inlined]
true_main at /buildworker/worker/package_linux64/build/src/jlapi.c:559
jl_repl_entrypoint at /buildworker/worker/package_linux64/build/src/jlapi.c:701
main at julia (unknown line)
__libc_start_main at /lib64/libc.so.6 (unknown line)
unknown function (ip: 0x400808)
Allocations: 35529115 (Pool: 35517210; Big: 11905); GC: 35

signal (11): Segmentation fault
in expression starting at /home/wyou/misc/git-julia/pde-on-gpu-wu/lecture8/scripts/debug/alltoall_test_cuda.jl:17
__memmove_avx_unaligned at /lib64/libc.so.6 (unknown line)
uct_mm_ep_am_short at /usr/lib64/libuct.so.0 (unknown line)
ucp_tag_send_nbx at /lib64/libucp.so.0 (unknown line)
MPIDI_UCX_send.constprop.0.isra.0 at /home/wyou/spack/opt/spack/linux-rocky8-zen2/gcc-11.2.0/mpich-4.0.2-nny23wg4ku5utyrhdtw7ccz3nlgqmmsg/lib/libmpi.so (unknown line)
MPIR_Sendrecv_impl at /home/wyou/spack/opt/spack/linux-rocky8-zen2/gcc-11.2.0/mpich-4.0.2-nny23wg4ku5utyrhdtw7ccz3nlgqmmsg/lib/libmpi.so (unknown line)
MPI_Sendrecv at /home/wyou/spack/opt/spack/linux-rocky8-zen2/gcc-11.2.0/mpich-4.0.2-nny23wg4ku5utyrhdtw7ccz3nlgqmmsg/lib/libmpi.so (unknown line)
Sendrecv! at /home/wyou/.julia/packages/MPI/08SPr/src/pointtopoint.jl:380 [inlined]
Sendrecv! at /home/wyou/.julia/packages/MPI/08SPr/src/pointtopoint.jl:389
unknown function (ip: 0x1554fa9e6acd)
_jl_invoke at /buildworker/worker/package_linux64/build/src/gf.c:2247 [inlined]
jl_apply_generic at /buildworker/worker/package_linux64/build/src/gf.c:2429
jl_apply at /buildworker/worker/package_linux64/build/src/julia.h:1788 [inlined]
do_call at /buildworker/worker/package_linux64/build/src/interpreter.c:126
eval_value at /buildworker/worker/package_linux64/build/src/interpreter.c:215
eval_stmt_value at /buildworker/worker/package_linux64/build/src/interpreter.c:166 [inlined]
eval_body at /buildworker/worker/package_linux64/build/src/interpreter.c:587
jl_interpret_toplevel_thunk at /buildworker/worker/package_linux64/build/src/interpreter.c:731
jl_toplevel_eval_flex at /buildworker/worker/package_linux64/build/src/toplevel.c:885
jl_toplevel_eval_flex at /buildworker/worker/package_linux64/build/src/toplevel.c:830
jl_toplevel_eval_in at /buildworker/worker/package_linux64/build/src/toplevel.c:944
eval at ./boot.jl:373 [inlined]
include_string at ./loading.jl:1196
_jl_invoke at /buildworker/worker/package_linux64/build/src/gf.c:2247 [inlined]
jl_apply_generic at /buildworker/worker/package_linux64/build/src/gf.c:2429
_include at ./loading.jl:1253
include at ./Base.jl:418
_jl_invoke at /buildworker/worker/package_linux64/build/src/gf.c:2247 [inlined]
jl_apply_generic at /buildworker/worker/package_linux64/build/src/gf.c:2429
exec_options at ./client.jl:292
_start at ./client.jl:495
jfptr__start_22567.clone_1 at /cm/shared/apps/easybuild-apps/software/Julia/1.7.3-linux-x86_64/lib/julia/sys.so (unknown line)
_jl_invoke at /buildworker/worker/package_linux64/build/src/gf.c:2247 [inlined]
jl_apply_generic at /buildworker/worker/package_linux64/build/src/gf.c:2429
jl_apply at /buildworker/worker/package_linux64/build/src/julia.h:1788 [inlined]
true_main at /buildworker/worker/package_linux64/build/src/jlapi.c:559
jl_repl_entrypoint at /buildworker/worker/package_linux64/build/src/jlapi.c:701
main at julia (unknown line)
__libc_start_main at /lib64/libc.so.6 (unknown line)
unknown function (ip: 0x400808)
Allocations: 35528548 (Pool: 35516640; Big: 11908); GC: 35

signal (11): Segmentation fault
in expression starting at /home/wyou/misc/git-julia/pde-on-gpu-wu/lecture8/scripts/debug/alltoall_test_cuda.jl:17
__memmove_avx_unaligned at /lib64/libc.so.6 (unknown line)
uct_mm_ep_am_short at /usr/lib64/libuct.so.0 (unknown line)
ucp_tag_send_nbx at /lib64/libucp.so.0 (unknown line)
MPIDI_UCX_send.constprop.0.isra.0 at /home/wyou/spack/opt/spack/linux-rocky8-zen2/gcc-11.2.0/mpich-4.0.2-nny23wg4ku5utyrhdtw7ccz3nlgqmmsg/lib/libmpi.so (unknown line)
MPIR_Sendrecv_impl at /home/wyou/spack/opt/spack/linux-rocky8-zen2/gcc-11.2.0/mpich-4.0.2-nny23wg4ku5utyrhdtw7ccz3nlgqmmsg/lib/libmpi.so (unknown line)
MPI_Sendrecv at /home/wyou/spack/opt/spack/linux-rocky8-zen2/gcc-11.2.0/mpich-4.0.2-nny23wg4ku5utyrhdtw7ccz3nlgqmmsg/lib/libmpi.so (unknown line)
Sendrecv! at /home/wyou/.julia/packages/MPI/08SPr/src/pointtopoint.jl:380 [inlined]
Sendrecv! at /home/wyou/.julia/packages/MPI/08SPr/src/pointtopoint.jl:389
unknown function (ip: 0x1554fa9e6b1d)
_jl_invoke at /buildworker/worker/package_linux64/build/src/gf.c:2247 [inlined]
jl_apply_generic at /buildworker/worker/package_linux64/build/src/gf.c:2429
jl_apply at /buildworker/worker/package_linux64/build/src/julia.h:1788 [inlined]
do_call at /buildworker/worker/package_linux64/build/src/interpreter.c:126
eval_value at /buildworker/worker/package_linux64/build/src/interpreter.c:215
eval_stmt_value at /buildworker/worker/package_linux64/build/src/interpreter.c:166 [inlined]
eval_body at /buildworker/worker/package_linux64/build/src/interpreter.c:587
jl_interpret_toplevel_thunk at /buildworker/worker/package_linux64/build/src/interpreter.c:731
jl_toplevel_eval_flex at /buildworker/worker/package_linux64/build/src/toplevel.c:885
jl_toplevel_eval_flex at /buildworker/worker/package_linux64/build/src/toplevel.c:830
jl_toplevel_eval_in at /buildworker/worker/package_linux64/build/src/toplevel.c:944
eval at ./boot.jl:373 [inlined]
include_string at ./loading.jl:1196
_jl_invoke at /buildworker/worker/package_linux64/build/src/gf.c:2247 [inlined]
jl_apply_generic at /buildworker/worker/package_linux64/build/src/gf.c:2429
_include at ./loading.jl:1253
include at ./Base.jl:418
_jl_invoke at /buildworker/worker/package_linux64/build/src/gf.c:2247 [inlined]
jl_apply_generic at /buildworker/worker/package_linux64/build/src/gf.c:2429
exec_options at ./client.jl:292
_start at ./client.jl:495
jfptr__start_22567.clone_1 at /cm/shared/apps/easybuild-apps/software/Julia/1.7.3-linux-x86_64/lib/julia/sys.so (unknown line)
_jl_invoke at /buildworker/worker/package_linux64/build/src/gf.c:2247 [inlined]
jl_apply_generic at /buildworker/worker/package_linux64/build/src/gf.c:2429
jl_apply at /buildworker/worker/package_linux64/build/src/julia.h:1788 [inlined]
true_main at /buildworker/worker/package_linux64/build/src/jlapi.c:559
jl_repl_entrypoint at /buildworker/worker/package_linux64/build/src/jlapi.c:701
main at julia (unknown line)
__libc_start_main at /lib64/libc.so.6 (unknown line)
unknown function (ip: 0x400808)
Allocations: 34728965 (Pool: 34717306; Big: 11659); GC: 34

===================================================================================
=   BAD TERMINATION OF ONE OF YOUR APPLICATION PROCESSES
=   PID 2207490 RUNNING AT racklette1
=   EXIT CODE: 139
=   CLEANING UP REMAINING PROCESSES
=   YOU CAN IGNORE THE BELOW CLEANUP MESSAGES
===================================================================================
YOUR APPLICATION TERMINATED WITH THE EXIT STRING: Segmentation fault (signal 11)
This typically refers to a problem with your application.
Please see the FAQ page for debugging suggestions
ERROR: failed process: Process(`/home/wyou/spack/opt/spack/linux-rocky8-zen2/gcc-11.2.0/mpich-4.0.2-nny23wg4ku5utyrhdtw7ccz3nlgqmmsg/bin/mpiexec -n 4 julia alltoall_test_cuda.jl`, ProcessExited(139)) [139]

Stacktrace:
 [1] pipeline_error
   @ ./process.jl:540 [inlined]
 [2] run(::Cmd; wait::Bool)
   @ Base ./process.jl:455
 [3] run(::Cmd)
   @ Base process.jl:453
 [4] (::var&quot;#1#2&quot;)(exe::Cmd)
   @ Main none:4
 [5] (::MPI.var&quot;#28#29&quot;{var&quot;#1#2&quot;})(cmd::Cmd)
   @ MPI ~/.julia/packages/MPI/08SPr/src/environment.jl:25
 [6] _mpiexec(fn::MPI.var&quot;#28#29&quot;{var&quot;#1#2&quot;})
   @ MPI ~/.julia/packages/MPI/08SPr/deps/deps.jl:6
 [7] mpiexec(fn::var&quot;#1#2&quot;)
   @ MPI ~/.julia/packages/MPI/08SPr/src/environment.jl:25
 [8] top-level scope
   @ none:4

</code></pre></details><h2 id="Problem:-No-solutions-after-certain-iteration-count"><a class="docs-heading-anchor" href="#Problem:-No-solutions-after-certain-iteration-count">Problem:  No solutions after certain iteration count</a><a id="Problem:-No-solutions-after-certain-iteration-count-1"></a><a class="docs-heading-anchor-permalink" href="#Problem:-No-solutions-after-certain-iteration-count" title="Permalink"></a></h2><p><strong>Status</strong></p><p>[ x ] Resolved </p><pre><code class="nohighlight hljs"> - normal behavior to be expected</code></pre><p>Using the 2D Hydro-mechanical solver for small R values, solution cannot be correctly plotted after a certain state has been reached. The solution can be correctly plotted until the 48th frame (included) as followed.</p><p><strong>Description</strong></p><p><code>R=0.5</code>, <code>t=0.2</code></p><p><img src="../assets/images/HydroMech2D_R0p5.gif" alt="2D wave"/></p><p>Where at the 48th frame the distribution of the parameters looks as followed</p><p><img src="../assets/images/000048.png" alt="2D wave"/></p><pre><code class="language-bash hljs">it = 1138, time = 3.307e-02 sec (@ T_eff = 24.00 GB/s) 
it = 1139, time = 3.316e-02 sec (@ T_eff = 24.00 GB/s) 
it = 1140, time = 3.313e-02 sec (@ T_eff = 24.00 GB/s) 
GKS: Rectangle definition is invalid in routine SET_WINDOW
GKS: Rectangle definition is invalid in routine CELLARRAY
invalid range
GKS: Rectangle definition is invalid in routine SET_WINDOW
GKS: Rectangle definition is invalid in routine CELLARRAY</code></pre><p>Then at the frames after the 48th frame it looks identical to the 49th frame. <img src="../assets/images/000049.png" alt="2D wave"/></p><h2 id="Problem:-Overhead-brought-by-using-MetaHydroMech.jl"><a class="docs-heading-anchor" href="#Problem:-Overhead-brought-by-using-MetaHydroMech.jl">Problem:  Overhead brought by using MetaHydroMech.jl</a><a id="Problem:-Overhead-brought-by-using-MetaHydroMech.jl-1"></a><a class="docs-heading-anchor-permalink" href="#Problem:-Overhead-brought-by-using-MetaHydroMech.jl" title="Permalink"></a></h2><p><strong>Status</strong></p><p>[x] Resolved       - by calling only one <code>compute!()</code> kernel      - use the <code>const</code> to fix the type instability problem of PTArray</p><p><strong>Description</strong></p><p>Using the same pattern as <code>JustRelax.jl</code>, we added the <code>MetaHydroMech.jl</code> to predefine the environment needed for the use of the <code>ParallelStencil.jl</code>. However this brought us a significant loss in the performance. Before it was around 24 GB/s after the code improvement.</p><pre><code class="language-bash hljs">it = 1, time = 1.653e+00 sec (@ T_eff = 9.00 GB/s) 
it = 2, time = 1.519e-01 sec (@ T_eff = 16.00 GB/s) 
it = 3, time = 3.360e-01 sec (@ T_eff = 12.00 GB/s) 
it = 4, time = 2.038e-01 sec (@ T_eff = 16.00 GB/s) 
it = 5, time = 2.077e-01 sec (@ T_eff = 16.00 GB/s) 
it = 6, time = 2.661e-01 sec (@ T_eff = 15.00 GB/s) 
it = 7, time = 2.399e-01 sec (@ T_eff = 14.00 GB/s) 
it = 8, time = 2.561e-01 sec (@ T_eff = 16.00 GB/s) 
it = 9, time = 2.614e-01 sec (@ T_eff = 16.00 GB/s) 
it = 10, time = 2.914e-01 sec (@ T_eff = 14.00 GB/s) 
it = 11, time = 2.558e-01 sec (@ T_eff = 16.00 GB/s) 
it = 12, time = 2.582e-01 sec (@ T_eff = 16.00 GB/s) 
it = 13, time = 2.649e-01 sec (@ T_eff = 15.00 GB/s) 
it = 14, time = 2.845e-01 sec (@ T_eff = 14.00 GB/s) 
it = 15, time = 2.564e-01 sec (@ T_eff = 16.00 GB/s) 
Test Summary:                              | Pass  Total
Reference test: HydroMech2D_incompressible |    5      5
     Testing HydroMech tests passed 
</code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../timeline/">« Time line</a><a class="docs-footer-nextpage" href="../hydromech/">HydroMech.jl »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.23 on <span class="colophon-date" title="Wednesday 7 December 2022 19:07">Wednesday 7 December 2022</span>. Using Julia version 1.8.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
